---
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="100")
opts_chunk$set(# echo=TRUE,
	             cache = TRUE,
               prompt = FALSE,
               tidy = FALSE,
               comment = NA,
               message = FALSE,
               warning = FALSE,
	             fig.align = "center",
	             fig.height = 5,
	             fig.width = 5)
opts_knit$set(width=100)
```

### 7.2 데이터 전처리   

#### 7.2.1 데이터 및 패키지 불러오기     

```{r}
library(data.table)
library(ggplot2)
library(dplyr)
library(reshape)
DIR = "D:\\CSV\\"
Womens = fread(paste0(DIR, "Womens-Clothing-E-Commerce-Reviews.csv"))
str(Womens)
```

### 7.3 리뷰 데이터 기본 분석  

#### 7.3.1 데이터 전처리   

- 연속형 변수의 범주화 - cut()   

```{r}
Age_G = cut(as.numeric(Womens$Age),
            breaks = seq(10,100,by = 10),
            include.lowest = TRUE,
            right = FALSE,
            labels = paste0(seq(10,90,by = 10),"th"))

Age_G
```

```{r}
Womens$Age_G = Age_G
summary(Womens$Age_G)
```

- 텍스트 데이터 전처리 방법   

```{r}
set.seed(1234)
SL = sample(1:nrow(Womens), nrow(Womens) * 0.2, replace = FALSE)
Womens_T = Womens[SL,]

library(tm)

TEXT = as.character(Womens_T$`Review Text`)
TEXT[1]

# 대소문자 변환
tolower(TEXT[1])

# 알파벳 이외의 문자열 제거
gsub("[A-Z]"," ",TEXT[1])
gsub("[a-z]"," ",TEXT[1])
gsub("[^a-z]"," ",TEXT[1])
```

#### 7.3.2 데이터 기본 분석  

- 데이터 집계 및 시각화   

```{r}
Product_Ranking = Womens %>%
  group_by(`Department Name`,`Class Name`,`Clothing ID`) %>%
  summarise(Count = n()) %>%
  arrange(-Count)

Product_Ranking
```

```{r}
Product_Ranking2 = Womens %>%
  filter(Age_G %in% c("20th","30th","40th","50th","60th", "70th")) %>%
  group_by(Age_G,`Department Name`,`Class Name`,`Clothing ID`) %>%
  summarise(Count = n()) %>%
  arrange(-Count) %>%
  ungroup() %>%
  group_by(Age_G) %>%
  top_n(n = 5,wt = Count) %>%
  mutate(Rank = row_number()) %>%
  arrange(Age_G)

Product_Ranking2

ggplot(Product_Ranking2) +
  geom_bar(aes(x = Rank, y = Count, fill = `Department Name`), 
           stat = 'identity') +
  geom_label(aes(x = Rank, y = 100, label = paste0(`Class Name`,"-",`Clothing ID`))) +
  scale_x_reverse(breaks = 1:7) +
  coord_flip() +
  theme_bw() +
  theme(legend.position = "bottom") +
  facet_wrap(~Age_G)
```

```{r}
ggplot(Womens) +
  geom_bar(aes(x = Rating)) +
  theme_bw()
```

```{r}
Rating_Ranking = Womens %>%
  group_by(`Clothing ID`,
           `Department Name`,`Class Name`) %>%
  summarise(Count = n(),
            Mean = mean(Rating)) %>%
  ungroup() %>%
  top_n(n = 100, wt = Count) %>%
  top_n(n = 10, wt = Mean)

Rating_Ranking

```


```{r}
Rating_Ranking_Total = Womens %>%
  group_by(`Clothing ID`,
           `Department Name`,`Class Name`) %>%
  summarise(Count = n(),
            Mean = mean(Rating)) 

ggplot(Rating_Ranking_Total) +
  geom_label(aes(x = Count, y = Mean, col = `Department Name`,
                label = `Clothing ID`)) +
  theme_bw() +
  theme(legend.position = "bottom")
```

- 리뷰 텍스트 마이닝   

```{r}
# install.packages("tm")
library(tm)
TEXT_tolower = tolower(TEXT)
CORPUS = Corpus(VectorSource(TEXT_tolower)) 
CORPUS_TM = tm_map(CORPUS,removePunctuation) 
CORPUS_TM = tm_map(CORPUS_TM, removeNumbers) 
CORPUS_TM = tm_map(CORPUS_TM,removeWords, c(stopwords("english")))

```

```{r}
TDM = TermDocumentMatrix(CORPUS_TM)
TDM_Matrix = as.matrix(TDM)

```

```{r}
Freq = rowSums(TDM_Matrix)

TERM_FREQ = data.frame(
  Words = names(Freq),
  Freq = Freq
)

TERM_FREQ = TERM_FREQ %>%
arrange(-Freq)

TERM_FREQ
```

- 키워드 토큰화 

```{r}
# install.packages(“tokenizers”)
library(tokenizers)
tokenize_word_stems(TEXT[1])

```

```{r}
Sentence = ""

for(tk in unlist(tokenize_word_stems(TEXT[1]))){
  
  Sentence = paste(Sentence, tk)
  
}

Sentence
```

```{r}
TEXT_Token = c()

for(i in 1:length(TEXT)){
  
  Words_token = unlist(tokenize_word_stems(TEXT[i]))
  
  Sentence = ""

  for(tk in Words_token){
    
    Sentence = paste(Sentence, tk)
    
  }
  
  TEXT_Token[i] = Sentence
}

```

```{r}
CORPUS_Token = Corpus(VectorSource(TEXT_Token)) 
CORPUS_TM_Token = tm_map(CORPUS_Token,removePunctuation) 
CORPUS_TM_Token = tm_map(CORPUS_TM_Token, removeNumbers) 
CORPUS_TM_Token = tm_map(CORPUS_TM_Token,removeWords, c(stopwords("english")))

TDM_Token = TermDocumentMatrix(CORPUS_TM_Token)
TDM_Matrix_Token = as.matrix(TDM_Token)
Freq_Token = rowSums(TDM_Matrix_Token)

TERM_FREQ_Token = data.frame(
  Words = names(Freq_Token),
  Freq = Freq_Token
)

TERM_FREQ_Token = TERM_FREQ_Token %>%
  arrange(-Freq)

nrow(TERM_FREQ)
nrow(TERM_FREQ_Token)
```

- 워드클라우드   

```{r}
# install.packages(“wordcloud”)
library(wordcloud)

wordcloud(words = TERM_FREQ_Token$Words,
          freq = TERM_FREQ_Token$Freq,
          max.words = 300,
          random.order = FALSE,
          random.color = TRUE,
          colors = brewer.pal(8, "Dark2"))
```

### 7.4 감성사전 생성을 위한 모델링

```{r}
Positive_Feedback_Analysis = Womens %>%
  mutate(Positive_Binary = ifelse(`Positive Feedback Count` > 0, 1, 0)) %>%
  select(`Positive Feedback Count`, Positive_Binary) 

Positive_DATA = Positive_Feedback_Analysis[SL,]
summary(as.factor(Positive_DATA$Positive_Binary))
```

#### 7.4.1 키워드 점수 계산을 위한 데이터셋 생성  

- DTM 생성   

```{r}
DTM_Token = DocumentTermMatrix(CORPUS_TM_Token)
DTM_Matrix_Token = as.matrix(DTM_Token)
```

- 상위 키워드 추출   

```{r}
Many_Words = colSums(DTM_Matrix_Token) > quantile(colSums(DTM_Matrix_Token), 
						probs = 0.99)

DTM_Matrix_Token_Selected = DTM_Matrix_Token[,Many_Words]
ncol(DTM_Matrix_Token_Selected)
DTM_DF_Token_Selected = as.data.frame(DTM_Matrix_Token_Selected)
Positive_DATA = cbind(Positive_DATA, DTM_DF_Token_Selected)
```

- 훈련/검증용 데이터 분류   

```{r}
set.seed(123)
SL2 = sample(1:nrow(Positive_DATA), nrow(Positive_DATA) * 0.7, replace = FALSE)

Positive_DATA_Train = Positive_DATA[SL2,]
Positive_DATA_Test = Positive_DATA[-SL2,]
```

#### 7.4.2 고차원 분류 모형  

- 변수 선택법   

```{r}
Start_Time = Sys.time()
GLM = step(glm(Positive_Binary ~ ., data = Positive_DATA_Train[,-1],
               family = binomial(link = "logit")),direction = "backward")

End_Time = Sys.time()

difftime(End_Time,Start_Time, unit = "secs")
```

```{r}
summary(GLM)
```

```{r}
library(pROC)
Predict_GLM = predict(GLM, newdata = Positive_DATA_Test,
                      type = 'response')
ROC_GLM = roc(Positive_DATA_Test$Positive_Binary, Predict_GLM)
plot.roc(ROC_GLM,
         print.auc = TRUE)

```


- Ridge, Lasso 회귀분석  

```{r}
# install.packages(glmnet)
library(glmnet)
x = as.matrix(Positive_DATA_Train[,c(-1,-2)])
y = as.matrix(Positive_DATA_Train[,2])
Start_Time = Sys.time()
Ridge = glmnet(x,y, alpha = 0,family = "binomial")
Lasso = glmnet(x,y, alpha = 1,family = "binomial")
End_Time = Sys.time()
difftime(End_Time,Start_Time, unit = "secs")

```

```{r}
Ridge_Beta = as.data.frame(t(as.matrix(Ridge$beta)))
Ridge_Beta$Lambda = Ridge$lambda

Ridge_Beta_M = Ridge_Beta %>%
  reshape::melt(id.vars = c("Lambda"))

ggplot(Ridge_Beta_M) +
  geom_line(aes(x = log(Lambda), y = value, group = variable, col = variable),
            alpha = 0.6) +
  guides(col = FALSE) +
  ylab("Coefficients") + xlab("Log Lambda") +
  scale_color_grey() +
  theme_bw() +
  theme(text = element_text(size = 15, face = "bold")) +
  ggtitle("Ridge Regreesion")

Lasso_Beta = as.data.frame(t(as.matrix(Lasso$beta)))
Lasso_Beta$Lambda = Lasso$lambda

Lasso_Beta_M = Lasso_Beta %>%
  reshape::melt(id.vars = c("Lambda"))

Lasso_Beta_M = Lasso_Beta %>%
  reshape::melt(id.vars = c("Lambda"))

ggplot(Lasso_Beta_M) +
  geom_line(aes(x = log(Lambda), y = value, group = variable, col = variable),
            alpha = 0.6) +
  guides(col = FALSE) +
  ylab("Coefficients") + xlab("Log Lambda") +
  scale_color_grey() +
  theme_bw() +
  theme(text = element_text(size = 15, face = "bold")) +
  ggtitle("Lasso Regreesion")

```

```{r}
Lambda = sort(unique(Lasso_Beta_M$Lambda))
Lambda_S = Lambda[c(1,15,35)]

Lasso_Beta_M %>%
  filter(Lambda %in% Lambda_S) %>%
  # group_by(Lambda) %>%
  top_n(n = 20, wt = value) %>%
  ggplot() +
  geom_bar(aes(x = reorder(variable,value), y = value), stat = 'identity') +
  facet_wrap(~Lambda) +
  xlab("Words") + ylab("Coefficient") +
  coord_flip() +
  theme_bw() +
  theme(axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 7.5))
```

- 최적의 모형 추정  

```{r}
DTM_DF_Token = as.data.frame(DTM_Matrix_Token)

ncol(DTM_DF_Token)
```

```{r}
Positive_Feedback_Analysis = Womens %>%
  mutate(Positive_Binary = ifelse(`Positive Feedback Count` > 0, 1, 0)) %>%
  select(`Positive Feedback Count`, Positive_Binary) 

Positive_DATA = Positive_Feedback_Analysis[SL,]
Positive_DATA = cbind(Positive_DATA, DTM_DF_Token)

set.seed(123)
SL2 = sample(1:nrow(Positive_DATA), nrow(Positive_DATA) * 0.7, replace = FALSE)

Positive_DATA_Train = Positive_DATA[SL2,]
Positive_DATA_Test = Positive_DATA[-SL2,]

set.seed(123)
CV_ID = sample(rep(seq(4), length=nrow(Positive_DATA_Train)))

x = as.matrix(Positive_DATA_Train[,c(-1,-2)])
y = as.matrix(Positive_DATA_Train[,2])

Start_Time = Sys.time()
CV_Lasso = cv.glmnet(x,y, alpha = 1, foldid = CV_ID, family = "binomial")
End_Time = Sys.time()
difftime(End_Time,Start_Time, unit = "secs")
```

```{r}
Lasso_Min = CV_Lasso$lambda.min
Lasso_1se = CV_Lasso$lambda.1se

Lasso_Optimal = glmnet(x,y, lambda = Lasso_Min, family = "binomial")


CV_Coef = as.vector(Lasso_Optimal$beta)
CV_Coef_Index = which(CV_Coef != 0)
CV_Coef2 = CV_Coef[CV_Coef_Index]

Lasso_Coef = data.frame(
  Predictors = CV_Coef_Index,
  Words = colnames(x)[CV_Coef_Index],
  Coefficients = CV_Coef2
)

knitr::kable(Lasso_Coef %>%
               arrange(-Coefficients) %>%
               top_n(n = 10))

knitr::kable(Lasso_Coef %>%
               arrange(Coefficients) %>%
               slice(1:10))
```

- 분류 모형의 성능 비교  

```{r}
x_test = as.matrix(Positive_DATA_Test[,c(-1,-2)])
y_test = as.matrix(Positive_DATA_Test[,2])

x_selected = x[,CV_Coef_Index]
x_test_selected = x_test[,CV_Coef_Index]
```

```{r}
GLM = glm(y ~ ., data = as.data.frame(x_selected),
               family = binomial())

GLM_Probs = predict(GLM, newdata = as.data.frame(x_test_selected),
                    type = "response")
```

```{r}
library(randomForest)

RF = randomForest(as.factor(y) ~ .,data = x_selected,
                   mtry = 30, ntree = 1000, importance = T)
```

```{r}
RF_Probs = predict(RF, x_test_selected, type = "prob")

# Lasso
Lasso_Probs = predict(Lasso_Optimal,newx = x_test, type = 'response')
```

```{r}
library(pROC)

GLM_ROC = roc(as.vector(y_test), GLM_Probs)
RF_ROC = roc(as.vector(y_test), RF_Probs[,1])
Lasso_ROC = roc(as.vector(y_test), Lasso_Probs[,1])

par(mfrow = c(1,3))
plot.roc(GLM_ROC,print.auc = TRUE,print.thres = TRUE,main = "Logistic Regression")
plot.roc(RF_ROC,print.auc = TRUE,print.thres = TRUE, main = "Random Forest")
plot.roc(Lasso_ROC,print.auc = TRUE,print.thres = TRUE, main = "Lasso Regression")
```

### 7.5 고객 리뷰 감성분석    

#### 7.5.1 감성분석 진행    

- 패키지를 활용한 감성분석   

```{r}
# install.packages(“syuzhet”)
library(syuzhet)
TEXT = Womens_T$`Review Text`
sentiment_vector = get_sentiment(TEXT, method="syuzhet")
summary(sentiment_vector)
```

```{r}
Womens_T$Sentiment = sentiment_vector

Womens_T %>%
  filter(`Department Name` != "") %>%
  ggplot() +
  geom_density(aes(x = Sentiment, fill = `Department Name`),
               alpha = 0.4) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  theme_bw() +
  guides(fill = guide_legend(nrow = 1)) +
  theme(legend.position = "bottom") +
  facet_wrap(~ `Department Name`)
```

- 생성한 감성사전을 활용한 감성분석  

```{r}
x_words = rbind(as.data.frame(x_selected),
                as.data.frame(x_test_selected))

Lasso_Sentiment = x_words * Lasso_Coef$Coefficients

Womens_T$Sentiment2 = rowSums(Lasso_Sentiment)

Womens_T %>%
  filter(`Department Name` != "") %>%
  ggplot() +
  geom_density(aes(x = Sentiment2, fill = `Department Name`),
               alpha = 0.4) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  theme_bw() +
  guides(fill = guide_legend(nrow = 1)) +
  theme(legend.position = "bottom") +
  facet_wrap(~ `Department Name`)
```
